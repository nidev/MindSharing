Chain Engine에서 사용하는 감정 어휘 분석을 위한 처리 과정이다.

간략히 전처리(preprocessing) => 관계 분석(identifying) 및 태그(tagging) => 결합(jointing) => 원시 이미지 설정(configuring) => 정제(Refining) => 수치화(quantifying) 및 데이터베이스 학습(learning with Database) => 결과 제공(reporting) 단계를 거쳐서 이루어진다.

# 변경사항

2014-2학기:
2차원 평면에서 각 감정 원소들의 위치를 표현할 수 있도록 감정 벡터를 (X, Y) 2차원으로 변환함.
데이터베이스에 학습 레코드를 저장할 수 있도록 시스템 구조 변경
기존의 감정값 모델은 자동으로 학습된 값을 바탕으로 판별하나,
현재 모델은 감정값 출력 확률을 적용, 세기(Amplification)으로 분할. 분류는 확률로, 세기는 확률에 log(10+세기)로 곱하여 출력을 내보냄. 출력함수는 0에서 '1'을 출력하고 무한대로 발산하는 모든 y=f(x) 모델 사용가능


2014-1학기:
Chain Engine(체인 엔진) 프로젝트 설계 및 개발

이미 제작된 형태소 분석기를 사용하고, 기계적으로 글을 읽고 이해하는 시스템을 구성하기로 한다. 장소, 시간, 감탄사 등을 최대한 배제하고 주어와 어떤 대상, 감정표현과의 관계를 위주로 파악하며, 주어를 중심으로 어휘 간의 관계를 인식하는 걸 목표로 제작한다.

2013-2학기:
SylphEngine(실프 엔진) 프로젝트 종료.
Lossy search(뒤의 단어를 삭제하며 체언을 찾는 알고리즘)로 어휘 사전을 단순하게 만들려고 시도했으나, 어휘 간의 관계 파악에는 많은 어려움이 있었다. 어휘 사전은 단순히 긍정 어휘와 부정 어휘로 나누어 구성했다.

# 분석 알고리즘 개요

한국어 문장의 표현은 복잡한 것처럼 보이지만, 실제로 초등학교 1학년 정도의 이해 수준을 생각하면 복잡한 문장에서도 핵심어를 얻어낼 수 있다.

유치원생은 선생님이 말한 문장을 모두 이해하는 것이 아니라, 앞에서의 전제 조건이나 준비 문구를 모두 생략한 채 마지막에 결정적인 핵심 표현만을 잡고 따라한다. 이해를 못하고 따라하는 경우에도, 반복되는 학습을 통해 앞에서 선생님이 말한 말이 어떤 의미인지 이해하게 되고 자연스럽게 문장의 이해 범위가 확장된다.

이때 아이들이 사용할 수 있는 간단한 표현들을 찾아보면,

	철수는 밥을 먹는다.
	나쁜 사람을 따라가면 안된다.
	착한 아이가 되어야한다.
	나는 철수입니다.
	얘가 나를 때렸다.
	기분이 어떻니? /  좋아요!
	친구가 친구를 때리는 것은 나쁘다.
	꽃이 참 예뻐요.
	너무 맛없어요.

일반적으로 '(주어)는 어떠하다.', '(주어)는 -되다.' 꼴로 간단히 이루어지는 표현이 많으며, '~~하는 것'과 같은 경우도 고려하면, 핵심적인 표현끼리는 비교적 결합이 간단함을 알 수 있다.

따라서 본 엔진에서는 약 5~6세 정도 어린이가 이해할 수 있는 수준의 문장 구성으로 한정하여 처리하고, 사전에 작성된 감정 어휘 데이터베이스를 통해 감정을 파악한 다음, 제한된 결합 조건을 사용하여 서술어와 서술어간의 관계를 파악할 것이다.

이런 제한조건을 적용한 것은, SNS에서 나타나는 단문 텍스트들에 복잡한 문장구조가 나타나지 않고, 문단의 길이가 그리 길지 않은 점을 고려한 것이다. 이번 캡스톤 프로젝트에서는 이러한 방식으로 한국어 텍스트를 접근할 때 충분히 정보를 얻을 수 있다고 가정하고, 실제 분석을 통해 결과를 얻어보는 것을 목표로 한다.

주요 분석 과정은 다음과 같다. 먼저 형태소 분석기를 사용해 한국어 조사를 사용해 각 명사 어휘, 체언의 품사를 확인하고 조사를 모두 삭제하며 어휘 객체에 조사의 의미를 갖고 있는 태그를 달고 있는 객체만을 남기는 전처리 단계를 수행한 후, 엔진에서 자체적으로 부여한 태그들을 바탕으로 제한된 결합조건을 사용해 텍스트와 텍스트가 의미를 가질 수 있도록 결합하는 작업을 수행한다. 그와 동시에, 미리 작성된 감정 어휘 사전을 사용하여 각 어휘의 감정값을 확인하고 연산 조건에 맞게 변형하고 정규화하여 최종적인 값을 추출하게 된다.

예를 들어, 위의 예문으로 든 '나쁜 사람을 따라가면 안된다'의 경우는 이 단계에서 다음과 같이 변환된다.

	[ 나쁘다(형용사) + 사람(객체) ]-<객체> [따라가다]-<행동> [면]-<다음 서술어 확인> [안된다]-<보조 행동어>

이렇게 변환된 배열을 탐색하면서 인과 관계를 파악하면 다음과 같이 다시 요약이 가능하다.

	[나쁜 사람] [따라가다] [그 행동은 안된다]

이 글에서 추출된 감정 형용사는 '나쁘다' 정도이지만, '행동을 금지한다'라는 행동을 함께 감정에 영향을 미치는 요소로 파악할 수 있다면 감정 표현에 용이할 것이다.

다른 예로, '친구가 친구를 때리는 것은 나쁘다.'이라는 표현에 안긴 표현을 봐보자.

	[친구]-<주어> [친구]-<목적어> [때리다]-<행동> [것은]-<문장 주어 표시> [나쁘다]-<형용사>

'것' 이라는 표현 때문에 앞의 주어로 시작하여 것까지 부분은 한 덩어리로 뭉쳐진다.


	[ [친구]-<주어> [친구]-<목적어> [때리다]-<행동> ]-<주어> [나쁘다]-<형용사>

마치 수학의 (5 * 2 + 3) + 5 와 같은 연산을 연상시킨다. 기본적으로 여기에서 사용하는 알고리즘은 국어를 수학적 연산을 모방한 언어의 연산으로 처리하는 것에 목표를 하고 있다. 이러한 처리 방법이 과연 트위터 실시간 분석에 필요한 연산 소요 시간을 만들어낼지는 실험을 통해 분석하게 된다.

기본적으로 감정값은 '감정적 긍정/부정 어휘 확률'과 '가시적 발전/쇠퇴'의 확률값으로 나타낸다. 각 확률의 절대값은 0에서 최대 1까지 값을 가질 수 있다.

이때 부정과 쇠퇴는 음수 부호를 사용해서 표현한다. 따라서 모든 어휘는 X [-1, 1], Y [-1, 1] 범위내에서 표현된다. 

이것들은 연구원과 다수의 설문 조사를 통하여 결정한 값이다. 기본 어휘는 20개, 50개, 100개, 300개 모델로 나눠진다. 기본 어휘에는 형용사, 명사, 동사로 이루어져있고, '-하다'와 같이 결합하는 동사는 명사로 간주한다.
이들 어휘가 갖고 있는 감정값을 '원시 감정확률'이라고 부른다. 원시 감정확률은 +1 또는 -1로 크리스피한 값을 가진다. 완벽하다고 간주하는 것이다.

기본 어휘 중 특별한 어휘인 '연산 어휘'는 '그러나, 그리고, 그래서, 그런데, 하지만'과 같은 접속사, '안-(아니-)'과 '않다'처럼 부정의 의미를 나타내는 서술어, '너무, 아주, 정말, 진짜'와 같이 감정의 세기를 변화시키는 부사로 구성된다.
 
새로 학습된 어휘는 기본 어휘와는 다르게 패키지 내부에 저장되는 게 아니라 외부 데이터베이스에 저장된다. 필요에 따라 리셋할 수 있다.

새로 학습된 어휘는 원시 감정확률이 아닌 추정 확률값을 가지게 되며, 추정 확률이 1이상 또는 -1이하가 되면 1과 -1로 각각 고정된다. 그리고 출현 빈도수를 고려해야하므로, 빈도수를 1회에 1씩 증가시키는 '세기(amplification)'를 도입한다. 따라서 기본 어휘에 비해 2가지 속성이 더 필요하다. 추가적으로 -1(실험을 위한 조정값 -0.910)과 1(실험을 위한 조정값 +0.910)에 도달한 후 더이상 값을 바꾸지않는 고정(Locking) 여부를 표현하는 boolean 속성이 필요하다. 고정 후에는 고정값을 소추정확률 레코드 테이블이 아닌 어휘 테이블에 반영한다.
고정된 어휘는 '세기'는 증가되는데 더이상 해당 단어에 대한 이미지를 바꾸지 않는 상태를 의미한다. 이것은 보통의 인간이 '공포'라는 단어에 좋지않은 이미지를 갖고 있고, 이걸 평생 가지고가는 것과 비슷하다. 고정된 확률에 배척되는 감정확률이 추론된 경우, 분석 과정에서 '모순'적 표현임을 추론할 수 있다.
 
'세기(amplification)'은 확률에 보조적으로 사용하는 모델이다. 기본적으로 확률에 영향은 미치지않으나, '감정값( EV = 확률 * f(amp), 여기에서 f는 0에서 항상 1을 갖고 무한대로 발산하는 미분이 가능한 함수이다.)'을 구하는데 사용된다. 확률이 0이라면 항상 0을 출력하게 된다.

분석과정에서 분석을 통해 구해낸 다른 어휘들의 추정확률과 기본 어휘의 확률을 동시에 사용한다. 이 과정은 재참조(re-reference)라고 부른다.

추정확률은 데이터베이스에서 해당 어휘에 대한, 시간 정보를 표현한 소추정확률 값의 정규화된 값이다. 소추정확률은 문장 전체에서 나타나는 원시감정/재참조된 추정확률값의 평균이다. 매 시간마다 새로 들어오는 데이터이므로, 데이터베이스의 한 레코드(record)에 해당하게 된다. 따라서 추정확률은 여러 레코드의 정규화된 값이라고 볼 수 있다.

소추정확률들은 데이터베이스로부터 n개의 레코드로 수신된다. n개의 레코드를 수신하면 평균, 분산, 표준 편차를 구한다.
1) 표준편차가 크다면 n개의 레코드 중 평균 이하인 것과 평균 이상인 값의 빈도를 계산한다. 이 중에 다수를 선정한다. 다수의 그룹에서 최저 확률은 0, 최대 확률을 1로 변환된 확률의 평균을 구한다. 이것의 평균을 정규화된 출력으로 내보낸다.
2) 표준편차가 충분히 작다면(실험으로 구해야함) 다수를 구하지 않고 최저확률을 0, 최대를 1로 설정하여 다시 평균을 구하는 작업만 반복한다. 이것을 정규화된 출력으로 내보낸다.
3) 충분히 세다면 (|p| >= 0.910 실험을 통해 테스트해야함) 이 과정에서 고정을 시도할 수 있다.

 
전처리 분석과 태깅을 마친 후 결합이 모두 완료되면, 서술어/형용사/명사(주어 목적어 포함)로 태그된 감정값을 문장 단위로 파악한다. 파악하는 과정은 다음과 같이 이루어진다.

1) 기본 어휘로 평가가 가능한 (이미 알고 있는) 단어는 빠르게 기본 어휘 사전을 통해 평가한다. 기본 어휘사전에 대한 세기값은, 기본 어휘를 위한 레코드 테이블에서 기억한다
2) 추정확률로 추정된 어휘는 추정확률 데이터베이스를 사용해 빠르게 평가한다.
2) 모르는  어휘가 발생한 경우, 문장 전체에 감정값이 나타난 부분이 있는지 체크한다.
    ---> 발생한 경우: 문장 평균 확률을 구하고 이것을 어휘 테이블에 추가한다. 그리고 소추정확률 테이블의 첫 레코드로 삽입한다.
3) 전체적인 감정흐름은 확률 벡터의 합으로 표현할 수 있다. 최종적으로 벡터가 도착한 지점이 문장의 감정위치이다. 세기를 고려하는 경우에는 합성시 감정값의 세기를 고려해서 벡터를 합성해야한다.

균형잡힌 자료를 학습 데이터로 넣고 추가적으로 분석이 이루어지는 경우에, 추정확률의 감정적 긍정/부정 확률을 x축 좌표, 가시적 긍정/부정 확률을 y축 좌표에 대입했을때,

                                _------_
           UNDEFINED          _         ---------
                            -  긍정어휘영역      ----_____
----------------------------|---------------------------------> x
 ---- _       부정어휘 영역 0_  
       _______             -      UNDEFINED
               -         -  
                ________-
                        
위와 같은 그래프를 y=x에 대칭시킨 그래프와 겹쳐나오는 그래프 형태가 예상된다. 근사적으로는 y = 1/x 의 안쪽 부분과 비슷하다.

세기를 고려한 경우에는 그래프 모양은 더욱 독특하게 그려질 것이다.

우세한 쪽을 구하는 것은 1사분면 또는 3사분면에 몰려있는 점들의 유클리드 거리의 합이다.

이 거리의 합을 구하는 경우, 분석을 위해 넣은 샘플이 어느 쪽으로 좀 더 치우쳤는지 알 수 있다. 긍정 영역/부정영역 비율을 통해 SNS 기상도를 그릴 수 있다. 1.2 이상이면 맑음, 0.8 이하면 비, 중간은 흐림으로 지정할 수 있다.
---------------

기본어휘의 테이블 애트리뷰트는 다음과 같다.
*어휘, 어휘id, 감정 +/-, 인과 +/-
연산 어휘의 테이블 애트리뷰트는 다음과 같다.
*어휘, 어휘id, 문장 inversion(부호 뒤집기) bool, 문장 감정세기강화 bool, 문장 감정세기 약화 bool 

 
# 처리 과정
## 전처리(preprocessing)

감정처리를 위해 입력되는 문장은, 사람이 읽고 쓰는 자연어이다. 이 문장 자체는 단순히 바이트의 배열에 그치지 않으므로, 의미 분석을 위해서는 형태소 분석을 해야하고, 그 전에 필요한 작업을 수행해야한다.

엔진에 데이터를 제공하기 전에, 클라이언트가 먼저 해결해주어야할 과제가 있다.

 * 링크 제거: 웹페이지 링크는 분석의 관심사가 아니기 때문에 제거한다.
 * 해시태그 제거: 해시태그는 정보를 분류하기위한 메타 데이터이므로 제거한다.
 * 한국어가 포함되지 않은 문장 제거: 외국어로 이루어진 문장은 처리하지않는다.
 * (트위터) @사용자이름 을 제거: 사용자 간의 대화에 관심이 있는 경우는 아니기 때문에, 제거한다.
 * 지나치게 짧은 글을 제거: 감탄사만 포함하거나, '밥 먹었음'처럼 지나치게 간결한 글은 분석 시간만 소비하므로 제외한다.

 형태소 분석기에서 위와 같은 내용은 충분히 걸러지기 때문에 별도로 필터링은 수행하지 않지만, 양질의 분석 결과를 원한다면 이렇게 걸러낸 SNS 글들을, 클라이언트가 엔진으로 한번에 묶어서 보내면 좋다. 이렇게 전송된 데이터는 엔진에서 따옴표를 해체하는 작업을 통해 알고리즘 적용에 무리가 없도록 수정된다. 전처리 작업이 모두 완료되면, 형태소 분석기를 사용해 형태소로 모두 나눈다.

## 관계 분석(identifying) 및 태그(tagging)

형태소 분석기로 분석된 어휘를, 분석기에서 파악한 품사에 맞춰 Chain Engine에서 사용하는 태그로 변환한다. 단, 이때는, EmoUnit.java 내부에 있는 WordTag에 포함된 모든 태그를 사용하는 건 아니다. 초기 분석을 위해 'Marker'로 끝나는 태그들을 사용한다.

엔진에서 사용하는 형태소 분석기는, 세종 말뭉치를 활용하여 만들어진, 기분석된 형태소 데이터를 바탕으로 분석하여 결과를 내주면서, 대부분 문법적인 요소를 그대로 갖고 왔기 때문에 감정 분석에 큰 관계가 없는 데이터가 많다.

따라서 주어진 조건에 따라, 내부의 마커로 변환하여여 '결합' 단계에서 어휘들이 잘 뭉쳐질 수 있도록 하는 편이 처리에 용이하다.

관계분석에서는 아래의 태그들을 바탕으로 분석하는데, 특히 감정값이 담긴 서술어 어휘를 중심으로 관계를 분석하여, 두 EmoUnit 객체를 결합한다.

(표로 다시 작성할 것)
* NextDescEnhancer: 다음에 오는 서술어의 감정값을 강화시킨다.
* NextDescReducer: 다음에 오는 서술어의 감정값을 약화시킨다.
* InvertNextDesc: 부정이나 반전의 의미를 담고 있는 태그로, 다음에 오는 서술어의 감정값을 반전시킨다.
* DescNextObject: 다음에 오는 주어나 목적어, 또는 명사 어휘 덩어리에 자신이 갖고 있는 감정값을 전달한다.

### 태그 변환 목록
괄호 안은 꼬꼬마 형태소 분석기의 태그이다.

편의상 결합 단계에서 사용하는 목적어 어휘(Object)와 주어 어휘(Subject) 태그를 미리 빌려 표현하기로 한다.

(표로 다시 작성해주기 바람!)
 * 부사(MAG): EQueryTool.java 에 정의된 함수를 사용해 어떤 마커를 설정할지 결정한다. 다음에 오는 감정 어휘의 값을 반전하는 InvertNextDesc 마커, 강화하는 NextDescEnhancer 마커, 감소하는 NextDescReducer 마커를 결정하며, 그 외의 모든 부사는 Skip 마커로 변한한다.
 * 동사/보조동사(VV/VXV/VX): 동사는 모두 VerbMarker로 태그한다. 원형이 순우리말 동사라면 동사 전체를, -하다 형이라면 앞의 '명사'만 남긴다. 그리고 VN으로 태그.(꼬꼬마태그 XR)
 * 형용사/보조형용사(VA/VXA): 형용사는 모두 AdjectMakrer로 태그한다.
 * 의존적 종결어미(ECD): NetxDescDepender 로 태그하고, 관계 분석 단계에서 다시 판단한다.
 * 관형사(MDT/MDN): DeterminerMarker로 태그하고, 결합 단계에서 명사들의 덩어리인 Object 또는 Subject 태그를 가진 객체에 합쳐진다. (미구현)
 * 문장마침(EFN): Skip으로 태그한다. 중요한 정보는 아니다.
 * 주어를 나타내는 조사(JKS/JX): SubjectTrailMarker로 태그하고, 결합 단계에서 이 앞에 나오는 Object나 NounMarker로 태그된 어휘를 주어로 가져간다.
 * 목적어를 나타내는 조사(JKO): ObjectTrailMarker로 태그하고, 명사 어휘들을 뭉쳐놓은 다음에 앞에 오는 Object를 목적어로 변경하기 위해 사용한다.
 * 인명과 고유명사를 포함한 모든 명사와 체언("NNG", "XR", "XSN", "NNP", "NNB", "NP"): NounMarker로 표시한다.

## 결합(jointing)

태그 단계에서 분석한 결과를 바탕으로 어휘와 어휘를 결합하고, 사용되지 않은 어휘를 제거한다.

이 단계에서 동사와 형용사는 서술어로 합쳐지며, 부사는 서술어의 강도에 영향을 미칠 수 있는 경우만 남는다. 그리고 주어를 Subject로 태그한다.

명사와 바로 인접한 명사는 합쳐져서 한 덩어리로 되고 Object로 태그된다. 만약에 Subject 태그가 이 단계에서 남지 않는 경우 휴리스틱을 사용해 가장 앞에 나오는 Object를 주어로 간주한다. 만약 없다면 '나'라는 가상의 주어를 삽입한다.


## 어휘 이미지 탐색(looking)

이미지 탐색은 사전에 정의된 기본 감정 어휘를 데이터베이스에서 찾아보는 단계이다. 관심있게 보는 어휘는, 동사와 형용사, 명사 어휘이다. 데이터베이스 내에서 각각 사람이 갖는 이미지와 사전적 정의를 바탕으로 미리 감정값이 정의되어있으므로, 이 단계에서는 감정값을 수신해주는 역할만 수행한다. 글의 흐름에 따른 감정값은 관계 분석 단계에서 찾는다.

감정을 나타내는 유형은 총 4가지로, 희비를 나타내는 JOY/SORROW, 복합적으로 긍정적인 결과를 낳을 것 같은 GROWTH, 부정적인 결과를 낳을 것 같은 CEASE 로 나누었다. 예를 들면, '즐겁다'라는  단어는 JOY에 가중치를 두고, '슬프다'라는 단어는 SORROW에 가중치를 둔다. '희망차다' 와 같은 표현은 GROWTH에 무게를 두고, '실패했다'와 같은 단어는 CEASE로 나누었다. 이와 같이 나눈 이유는, 인간의 감정은 단순히 감정의 기복으로 일어나는 것 뿐만 아니라, 외부 요인에 의해서 바뀌는 경우도 존재하기 때문이다.

데이터베이스에 사전 등록된 감정어휘가 갖고 있는 감정값을 원시 감정값이라 정의한다. 단계는 총 5단계로 None(감정값이 없음), Formal(격식있는 표현), Mild(일상적인 표현), Strong(강한 감정 표현), Extreme(극단적인 감정 표현)으로 나누어져있다. 기본적으로 이들은 Enumerate 타입으로 선언되어있지만, 처리와 게산을 용이하게 하기위해 0부터 4까지의 정수로 대응시킨다. 

대부분의 감정 어휘는 None과 Mild 사이에서 감정값이 결정되어있어야한다. 일부 극단적인 표현(ex. '죽다'의 속어 '뒈지다')에만 제한적으로 원시 감정값에 Strong을 허용해야한다.

원시 감정값들은, 그 앞에 어떤 표현이 오는지, 그 뒤에 역접하는 표현이 오는지 여부에 따라 감정값이 오르거나 내려가고, 바뀌기도 한다. 이런 경우를 대비해서 가능한 Strong 원시 감정값을 주지 않는다. 원시 감정값을 위해 Enumerate 타입을 사용한 고로, Extreme보다 더 큰 감정 크기는 나타나지 않는다. 이는 다른 말로, 화자의 발화가 이루어질 때 배경 상황이나 분위기를 고려하지않겠다는 제약 조건을 추가한 것이다.

원시 감정값이 아닌, 문맥 감정값이나 학습 모듈에서 사용하는 감정값은 double 타입의 실수값을 사용한다. 미세한 값 조절을 위해서이다.

결론적으로, 어휘 이미지 탐색 단계에서는 원시 감정값과 학습 모듈을 모두 참고하여 이미지 값을 수신한다. 이때, 탐색된 결과는 원시 감정값(정수)로 바뀌어야하므로, 현재는 학습 모듈에서 받아온 값을 반올림하여 정수로 가공하여 사용하고 있다.


위의 과정을 통해, 배열 내부에는 대부분의 서술어, 연산 태그, 마커 등이 사라진다. 그리고 의미를 갖는 덩어리로 완성되게 된다. 이러한 덩어리를, 본 프로젝트에서는 Nuri 라는 이름의 객체로 정리해, 재구성하는 단계로 이동한다.

## 재구성(refactoring)

재구성 단계에서는 이전 단계에서 분석된 결과를 바탕으로, 주어를 찾아서 Nuri 객체 내의 주어로 만들고, 감정값이 존재하거나, 감정값은 없지만 감정에 영향을 미칠 수 있는 서술어를 중심으로 새로운 ArrayList를 형성한다.

재구성을 함으로써, 텍스트 데이터 감정분석에 필요없다고 생각한 단어들을 모두 제거할 수 있어 효율적인 분석을 가능하게 한다. 만약 필요한 단어 중에 주어가 아닌 어휘들이 있다면, 이 어휘들은 Relations 라고 불리는 ArrayList 내에 보관된다.

## 수치화(quantifying) 

Nuri 객체의 Relations 안에 들어있는 어휘들의 감정값들은, 정수가 아닌 Enumerate 타입으로 정의되어있다. 따라서 이 Enum 값들을 다시 정수로 변환해야하는 과정이 필요하다. 왜냐하면 Java의 Enum값을 그대로 외부 API로 전달하기에는 부적합하기 때문이다.

## 요약(summarizing)

Nuri 객체 내부에 있는 감정 어휘를 일일이 매번 체크할 수 없으므로, 문장 전체를 대표할 수 있는 평균적인 감정값을 제공해야한다.

가장 간단한 방법으로, 객체 내부의 감정 어휘가 가진 감정값을 종류 별로 모두 더한 다음에, 문장 내 단어 수로 나누어 제공한다.

(공식)       { JOY, SORROW, GROWTH, CEASE 각각 감정 어휘의 감정값 합 }
       ----------------------------------------------------------------
                        (주어를 포함한 감정 어휘 수)

제공된 값은 소수점 5번째 자리까지 제공하기로 하고, 6번째에서 반올림한다. 

## 학습(feeding back) 


이 작업에서 피드백된 결과는 다시 데이터베이스의 별도의 테이블로 되돌려보낸다.
매번 학습된 결과는 데이터베이스내에 각 단어별로 각 행을 새롭게 구성하며, 감정값을 가져올 때는 이 값들을 모두 테이블에서 가져와 종합적인 값을 제공한다.
각 값은 시간에 비례하여 감소되는, 역비례 관계를 사용해 변환하여 사용한다.

시간이 흐를수록 사람의 기억은 사라지며, 감정에 대한 정도도 감소하기 때문이다. 이렇게 하여, 과거에 대한 기억을 조금씩 잊고 새로운 형태를 반영할 수 있는 지능을 갖게 된다.

구현은 메모리상에서만 학습을 하는 코드로, 엔진 서버를 재시작하면 초기화된다. 요약하자면, 데이터베이스에 돌려보내는 과정을 수행하지않는다.

## 결과 제공(reporting)

ResultProcessor 생성자에 EmotionAlgorithm 객체를 넘겨주면서 JSON데이터를 생성한다. EmotionAlgorithm 객체 속에는, 잘 정제된 감정 어휘 덩어리인 Nuri 객체들이 들어있다. 이 객체들을 처리하면서, ResultProcessor는 사용자가 보기 쉬운, 또는 API를 통해 웹에서 이용하기 쉬운 형태로 가공하게 된다.

추가적으로, 종합적인 분석 결과를 제공함과 동시에, 어떤 감정 어휘가 발견되었는지 기록을 남겨, 어플리케이션에서도 어떤 어휘가 영향을 미쳤는지 확인할 수 있도록 하였다. 이 프로젝트에서는 특히, 웹 인터페이스에 무게를 두고, JSP와 AJAX를 사용해 서버와 통신할 수 있도록 한다.
 