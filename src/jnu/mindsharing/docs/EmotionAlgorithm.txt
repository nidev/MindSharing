Chain Engine에서 사용하는 감정 어휘 분석을 위한 처리 과정이다.

간략히 전처리(preprocessing) => 태그(tagging) => 결합(jointing) => 어휘 이미지 탐색(looking) => 관계 분석(identifying) => 재구성(refactoring) => 수치화(quantifying) => 요약(summarizing) => 학습(feeding back) => 결과 제공(reporting) 단계를 거쳐서 이루어진다.

# 변경사항

2014-1학기:
Chain Engine(체인 엔진) 프로젝트 설계 및 개발

이미 제작된 형태소 분석기를 사용하고, 기계적으로 글을 읽고 이해하는 시스템을 구성하기로 한다. 장소, 시간, 감탄사 등을 최대한 배제하고 주어와 어떤 대상, 감정표현과의 관계를 위주로 파악하며, 주어를 중심으로 어휘 간의 관계를 인식하는 걸 목표로 제작한다.

2013-2학기:
SylphEngine(실프 엔진) 프로젝트 종료.
Lossy search(뒤의 단어를 삭제하며 체언을 찾는 알고리즘)로 어휘 사전을 단순하게 만들려고 시도했으나, 어휘 간의 관계 파악에는 많은 어려움이 있었다. 어휘 사전은 단순히 긍정 어휘와 부정 어휘로 나누어 구성했다.

# 분석 알고리즘 개요

한국어 문장의 표현은 복잡한 것처럼 보이지만, 실제로 초등학교 1학년 정도의 이해 수준을 생각하면 복잡한 문장에서도 핵심어를 얻어낼 수 있다.

유치원생은 선생님이 말한 문장을 모두 이해하는 것이 아니라, 앞에서의 전제 조건이나 준비 문구를 모두 생략한 채 마지막에 결정적인 핵심 표현만을 잡고 따라한다. 이해를 못하고 따라하는 경우에도, 반복되는 학습을 통해 앞에서 선생님이 말한 말이 어떤 의미인지 이해하게 되고 자연스럽게 문장의 이해 범위가 확장된다.

이때 아이들이 사용할 수 있는 간단한 표현들을 찾아보면,

	철수는 밥을 먹는다.
	나쁜 사람을 따라가면 안된다.
	착한 아이가 되어야한다.
	나는 철수입니다.
	얘가 나를 때렸다.
	기분이 어떻니? /  좋아요!
	친구가 친구를 때리는 것은 나쁘다.
	꽃이 참 예뻐요.
	너무 맛없어요.

일반적으로 '(주어)는 어떠하다.', '(주어)는 -되다.' 꼴로 간단히 이루어지는 표현이 많으며, '~~하는 것'과 같은 경우도 고려하면, 핵심적인 표현끼리는 비교적 결합이 간단함을 알 수 있다.

따라서 본 엔진에서는 약 5~6세 정도 어린이가 이해할 수 있는 수준의 문장 구성으로 한정하여 처리하고, 사전에 작성된 감정 어휘 데이터베이스를 통해 감정을 파악한 다음, 제한된 결합 조건을 사용하여 서술어와 서술어간의 관계를 파악할 것이다.

이런 제한조건을 적용한 것은, SNS에서 나타나는 단문 텍스트들에 복잡한 문장구조가 나타나지 않고, 문단의 길이가 그리 길지 않은 점을 고려한 것이다. 이번 캡스톤 프로젝트에서는 이러한 방식으로 한국어 텍스트를 접근할 때 충분히 정보를 얻을 수 있다고 가정하고, 실제 분석을 통해 결과를 얻어보는 것을 목표로 한다.

주요 분석 과정은 다음과 같다. 먼저 형태소 분석기를 사용해 한국어 조사를 사용해 각 명사 어휘, 체언의 품사를 확인하고 조사를 모두 삭제하며 어휘 객체에 조사의 의미를 갖고 있는 태그를 달고 있는 객체만을 남기는 전처리 단계를 수행한 후, 엔진에서 자체적으로 부여한 태그들을 바탕으로 제한된 결합조건을 사용해 텍스트와 텍스트가 의미를 가질 수 있도록 결합하는 작업을 수행한다. 그와 동시에, 미리 작성된 감정 어휘 사전을 사용하여 각 어휘의 감정값을 확인하고 연산 조건에 맞게 변형하고 정규화하여 최종적인 값을 추출하게 된다.

예를 들어, 위의 예문으로 든 '나쁜 사람을 따라가면 안된다'의 경우는 이 단계에서 다음과 같이 변환된다.

	[ 나쁘다(형용사) + 사람(객체) ]-<객체> [따라가다]-<행동> [면]-<다음 서술어 확인> [안된다]-<보조 행동어>

이렇게 변환된 배열을 탐색하면서 인과 관계를 파악하면 다음과 같이 다시 요약이 가능하다.

	[나쁜 사람] [따라가다] [그 행동은 안된다]

이 글에서 추출된 감정 형용사는 '나쁘다' 정도이지만, '행동을 금지한다'라는 행동을 함께 감정에 영향을 미치는 요소로 파악할 수 있다면 감정 표현에 용이할 것이다.

다른 예로, '친구가 친구를 때리는 것은 나쁘다.'이라는 표현에 안긴 표현을 봐보자.

	[친구]-<주어> [친구]-<목적어> [때리다]-<행동> [것은]-<문장 주어 표시> [나쁘다]-<형용사>

'것' 이라는 표현 때문에 앞의 주어로 시작하여 것까지 부분은 한 덩어리로 뭉쳐진다.


	[ [친구]-<주어> [친구]-<목적어> [때리다]-<행동> ]-<주어> [나쁘다]-<형용사>

마치 수학의 (5 * 2 + 3) + 5 와 같은 연산을 연상시킨다. 기본적으로 여기에서 사용하는 알고리즘은 국어를 수학적 연산을 모방한 언어의 연산으로 처리하는 것에 목표를 하고 있다. 이러한 처리 방법이 과연 트위터 실시간 분석에 필요한 연산 소요 시간을 만들어낼지는 실험을 통해 분석하게 된다.


# 처리 과정
## 전처리(preprocessing)

감정처리를 위해 입력되는 문장은, 사람이 읽고 쓰는 자연어이다. 이 문장 자체는 단순히 바이트의 배열에 그치지 않으므로, 의미 분석을 위해서는 형태소 분석을 해야하고, 그 전에 필요한 작업을 수행해야한다.

엔진에 데이터를 제공하기 전에, 클라이언트가 먼저 해결해주어야할 과제가 있다.

 * 링크 제거: 웹페이지 링크는 분석의 관심사가 아니기 때문에 제거한다.
 * 해시태그 제거: 해시태그는 정보를 분류하기위한 메타 데이터이므로 제거한다.
 * 한국어가 포함되지 않은 문장 제거: 외국어로 이루어진 문장은 처리하지않는다.
 * (트위터) @사용자이름 을 제거: 사용자 간의 대화에 관심이 있는 경우는 아니기 때문에, 제거한다.
 * 지나치게 짧은 글을 제거: 감탄사만 포함하거나, '밥 먹었음'처럼 지나치게 간결한 글은 분석 시간만 소비하므로 제외한다.

 형태소 분석기에서 위와 같은 내용은 충분히 걸러지기 때문에 별도로 필터링은 수행하지 않지만, 양질의 분석 결과를 원한다면 이렇게 걸러낸 SNS 글들을, 클라이언트가 엔진으로 한번에 묶어서 보내면 좋다. 이렇게 전송된 데이터는 엔진에서 따옴표를 해체하는 작업을 통해 알고리즘 적용에 무리가 없도록 수정된다. 전처리 작업이 모두 완료되면, 형태소 분석기를 사용해 형태소로 모두 나눈다.

## 태그(tagging)

형태소 분석기로 분석된 어휘를, 분석기에서 파악한 품사에 맞춰 Chain Engine에서 사용하는 태그로 변환한다. 단, 이때는, EmoUnit.java 내부에 있는 WordTag에 포함된 모든 태그를 사용하는 건 아니다. 초기 분석을 위해 'Marker'로 끝나는 태그들을 사용한다.

엔진에서 사용하는 형태소 분석기는, 세종 말뭉치를 활용하여 만들어진, 기분석된 형태소 데이터를 바탕으로 분석하여 결과를 내주면서, 대부분 문법적인 요소를 그대로 갖고 왔기 때문에 감정 분석에 큰 관계가 없는 데이터가 많다.

따라서 주어진 조건에 따라, 내부의 마커로 변환하여여 '결합' 단계에서 어휘들이 잘 뭉쳐질 수 있도록 하는 편이 처리에 용이하다.

### 태그 변환 목록
괄호 안은 꼬꼬마 형태소 분석기의 태그이다.

편의상 결합 단계에서 사용하는 목적어 어휘(Object)와 주어 어휘(Subject) 태그를 미리 빌려 표현하기로 한다.

(표로 다시 작성해주기 바람!)
 * 부사(MAG): EQueryTool.java 에 정의된 함수를 사용해 어떤 마커를 설정할지 결정한다. 다음에 오는 감정 어휘의 값을 반전하는 InvertNextDesc 마커, 강화하는 NextDescEnhancer 마커, 감소하는 NextDescReducer 마커를 결정하며, 그 외의 모든 부사는 Skip 마커로 변한한다.
 * 동사/보조동사(VV/VXV/VX): 동사는 모두 VerbMarker로 태그한다.
 * 형용사/보조형용사(VA/VXA): 형용사는 모두 AdjectMakrer로 태그한다.
 * 의존적 종결어미(ECD): NetxDescDepender 로 태그하고, 관계 분석 단계에서 다시 판단한다.
 * 관형사(MDT/MDN): DeterminerMarker로 태그하고, 결합 단계에서 명사들의 덩어리인 Object 또는 Subject 태그를 가진 객체에 합쳐진다. (미구현)
 * 문장마침(EFN): Skip으로 태그한다. 중요한 정보는 아니다.
 * 주어를 나타내는 조사(JKS/JX): SubjectTrailMarker로 태그하고, 결합 단계에서 이 앞에 나오는 Object나 NounMarker로 태그된 어휘를 주어로 가져간다.
 * 목적어를 나타내는 조사(JKO): ObjectTrailMarker로 태그하고, 명사 어휘들을 뭉쳐놓은 다음에 앞에 오는 Object를 목적어로 변경하기 위해 사용한다.
 * 인명과 고유명사를 포함한 모든 명사와 체언("NNG", "XR", "XSN", "NNP", "NNB", "NP"): NounMarker로 표시한다.

## 결합(jointing)

태그 단계에서 분석한 결과를 바탕으로 어휘와 어휘를 결합하고, 사용되지 않은 어휘를 제거한다.

이 단계에서 동사와 형용사는 서술어로 합쳐지며, 부사는 서술어의 강도에 영향을 미칠 수 있는 경우만 남는다. 그리고 주어를 Subject로 태그한다.

명사와 바로 인접한 명사는 합쳐져서 한 덩어리로 되고 Object로 태그된다. 만약에 Subject 태그가 이 단계에서 남지 않는 경우 휴리스틱을 사용해 가장 앞에 나오는 Object를 주어로 간주한다. 만약 없다면 '나'라는 가상의 주어를 삽입한다.


## 어휘 이미지 탐색(looking)

이미지 탐색은 사전에 정의된 기본 감정 어휘를 데이터베이스에서 찾아보는 단계이다. 관심있게 보는 어휘는, 동사와 형용사, 명사 어휘이다. 데이터베이스 내에서 각각 사람이 갖는 이미지와 사전적 정의를 바탕으로 미리 감정값이 정의되어있으므로, 이 단계에서는 감정값을 수신해주는 역할만 수행한다. 글의 흐름에 따른 감정값은 관계 분석 단계에서 찾는다.

감정을 나타내는 유형은 총 4가지로, 희비를 나타내는 JOY/SORROW, 복합적으로 긍정적인 결과를 낳을 것 같은 GROWTH, 부정적인 결과를 낳을 것 같은 CEASE 로 나누었다. 예를 들면, '즐겁다'라는  단어는 JOY에 가중치를 두고, '슬프다'라는 단어는 SORROW에 가중치를 둔다. '희망차다' 와 같은 표현은 GROWTH에 무게를 두고, '실패했다'와 같은 단어는 CEASE로 나누었다. 이와 같이 나눈 이유는, 인간의 감정은 단순히 감정의 기복으로 일어나는 것 뿐만 아니라, 외부 요인에 의해서 바뀌는 경우도 존재하기 때문이다.

데이터베이스에 사전 등록된 감정어휘가 갖고 있는 감정값을 원시 감정값이라 정의한다. 단계는 총 5단계로 None(감정값이 없음), Formal(격식있는 표현), Mild(일상적인 표현), Strong(강한 감정 표현), Extreme(극단적인 감정 표현)으로 나누어져있다. 기본적으로 이들은 Enumerate 타입으로 선언되어있지만, 처리와 게산을 용이하게 하기위해 0부터 4까지의 정수로 대응시킨다. 

대부분의 감정 어휘는 None과 Mild 사이에서 감정값이 결정되어있어야한다. 일부 극단적인 표현(ex. '죽다'의 속어 '뒈지다')에만 제한적으로 원시 감정값에 Strong을 허용해야한다.

원시 감정값들은, 그 앞에 어떤 표현이 오는지, 그 뒤에 역접하는 표현이 오는지 여부에 따라 감정값이 오르거나 내려가고, 바뀌기도 한다. 이런 경우를 대비해서 가능한 Strong 원시 감정값을 주지 않는다. 원시 감정값을 위해 Enumerate 타입을 사용한 고로, Extreme보다 더 큰 감정 크기는 나타나지 않는다. 이는 다른 말로, 화자의 발화가 이루어질 때 배경 상황이나 분위기를 고려하지않겠다는 제약 조건을 추가한 것이다.

원시 감정값이 아닌, 문맥 감정값이나 학습 모듈에서 사용하는 감정값은 double 타입의 실수값을 사용한다. 미세한 값 조절을 위해서이다.

결론적으로, 어휘 이미지 탐색 단계에서는 원시 감정값과 학습 모듈을 모두 참고하여 이미지 값을 수신한다. 이때, 탐색된 결과는 원시 감정값(정수)로 바뀌어야하므로, 현재는 학습 모듈에서 받아온 값을 반올림하여 정수로 가공하여 사용하고 있다.

## 관계 분석(identifying)

관계분석에서는 아래의 태그들을 바탕으로 분석하는데, 특히 감정값이 담긴 서술어 어휘를 중심으로 관계를 분석하여, 두 EmoUnit 객체를 결합한다.

(표로 다시 작성할 것)
* NextDescEnhancer: 다음에 오는 서술어의 감정값을 강화시킨다.
* NextDescReducer: 다음에 오는 서술어의 감정값을 약화시킨다.
* InvertNextDesc: 부정이나 반전의 의미를 담고 있는 태그로, 다음에 오는 서술어의 감정값을 반전시킨다.
* DescNextObject: 다음에 오는 주어나 목적어, 또는 명사 어휘 덩어리에 자신이 갖고 있는 감정값을 전달한다.

위의 과정을 통해, 배열 내부에는 대부분의 서술어, 연산 태그, 마커 등이 사라진다. 그리고 의미를 갖는 덩어리로 완성되게 된다. 이러한 덩어리를, 본 프로젝트에서는 Nuri 라는 이름의 객체로 정리해, 재구성하는 단계로 이동한다.

## 재구성(refactoring)

재구성 단계에서는 이전 단계에서 분석된 결과를 바탕으로, 주어를 찾아서 Nuri 객체 내의 주어로 만들고, 감정값이 존재하거나, 감정값은 없지만 감정에 영향을 미칠 수 있는 서술어를 중심으로 새로운 ArrayList를 형성한다.

재구성을 함으로써, 텍스트 데이터 감정분석에 필요없다고 생각한 단어들을 모두 제거할 수 있어 효율적인 분석을 가능하게 한다. 만약 필요한 단어 중에 주어가 아닌 어휘들이 있다면, 이 어휘들은 Relations 라고 불리는 ArrayList 내에 보관된다.

## 수치화(quantifying) 

Nuri 객체의 Relations 안에 들어있는 어휘들의 감정값들은, 정수가 아닌 Enumerate 타입으로 정의되어있다. 따라서 이 Enum 값들을 다시 정수로 변환해야하는 과정이 필요하다. 왜냐하면 Java의 Enum값을 그대로 외부 API로 전달하기에는 부적합하기 때문이다.

## 요약(summarizing)

Nuri 객체 내부에 있는 감정 어휘를 일일이 매번 체크할 수 없으므로, 문장 전체를 대표할 수 있는 평균적인 감정값을 제공해야한다.

가장 간단한 방법으로, 객체 내부의 감정 어휘가 가진 감정값을 종류 별로 모두 더한 다음에, 문장 내 단어 수로 나누어 제공한다.

(공식)       { JOY, SORROW, GROWTH, CEASE 각각 감정 어휘의 감정값 합 }
       ----------------------------------------------------------------
                        (주어를 포함한 감정 어휘 수)

제공된 값은 소수점 5번째 자리까지 제공하기로 하고, 6번째에서 반올림한다. 

## 학습(feeding back) 


이 작업에서 피드백된 결과는 다시 데이터베이스의 별도의 테이블로 되돌려보낸다.
매번 학습된 결과는 데이터베이스내에 각 단어별로 각 행을 새롭게 구성하며, 감정값을 가져올 때는 이 값들을 모두 테이블에서 가져와 종합적인 값을 제공한다.
각 값은 시간에 비례하여 감소되는, 역비례 관계를 사용해 변환하여 사용한다.

시간이 흐를수록 사람의 기억은 사라지며, 감정에 대한 정도도 감소하기 때문이다. 이렇게 하여, 과거에 대한 기억을 조금씩 잊고 새로운 형태를 반영할 수 있는 지능을 갖게 된다.

구현은 메모리상에서만 학습을 하는 코드로, 엔진 서버를 재시작하면 초기화된다. 요약하자면, 데이터베이스에 돌려보내는 과정을 수행하지않는다.

## 결과 제공(reporting)

ResultProcessor 생성자에 EmotionAlgorithm 객체를 넘겨주면서 JSON데이터를 생성한다. EmotionAlgorithm 객체 속에는, 잘 정제된 감정 어휘 덩어리인 Nuri 객체들이 들어있다. 이 객체들을 처리하면서, ResultProcessor는 사용자가 보기 쉬운, 또는 API를 통해 웹에서 이용하기 쉬운 형태로 가공하게 된다.

추가적으로, 종합적인 분석 결과를 제공함과 동시에, 어떤 감정 어휘가 발견되었는지 기록을 남겨, 어플리케이션에서도 어떤 어휘가 영향을 미쳤는지 확인할 수 있도록 하였다. 이 프로젝트에서는 특히, 웹 인터페이스에 무게를 두고, JSP와 AJAX를 사용해 서버와 통신할 수 있도록 한다.
 