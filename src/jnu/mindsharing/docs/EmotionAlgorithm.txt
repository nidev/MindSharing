
감정 어휘 분석은 다음과 같이 이루어진다.
(이루어지지않은 줄은 ++로 마크!)

1단계 문단의 문장화, 따옴표 해체
2단계 접속사와 문장 분리
3단계 문단, 문장, 감정 단어를 표현할 수 있는 모든 객체 생성
4단계 어휘 태깅
5단계 태그된 어휘에 대한 연산
6단계 감정값 정규화
7단계 분석 결과 JSON객체로 제공
8단계 데이터베이스에 피드백, 학습

상세 단계:

1단계:
++문단에서 따옴표를 모두 제거한다.
문단을 마침표, 물음표, 느낌표 기준으로 잘라내어 토큰을 만든다.

예) 문단: "나는 '밥'을 먹었다. 그러나 배가 고프다. "
   토큰 리스트: "나는 밥을 먹었다", "그러나 배가 고프다"
$$반환값$$ 토큰들이 저장된, 문장들의 어레이 리스트

2단계:
접속사를 문장에서 분리한다. 접속사 자체는 하나의 ESentence객체로 들어가며(문장 하나로 취급함), 특별한 의미를 갖는다.

$$반환값$$접속사와 문장이 분리되어 저장된, 문장들의 어레이 리스트

3단계:
문장을 표현하는 ESentence 객체를 생성하고, EParagraph 객체 안으로 넣는다.
EParagraph 객체는 여러개의 ESentence 객체를 갖는다. (왜냐면 문단은 문장의 집합이므로)

$$반환값$$ 아직 루프임

4단계:어휘 태깅 및 
ESentence 객체는 생성 당시 비어있게 된다. 이것을 감정처리 알고리즘이 돌면서 EmoUnit이라고 부르는, 감정 유닛 객체를 채워넣으면서 완성하게 된다. EmoUnit은 생성시 감정값이 0으로 초기화되어있다.

어휘는 EmoUnit에 있는 태깅 방법에 따라 분류된다. 이터레이터를 사용해 다음 형태소와 이전 형태소를 읽으며 각 단어를 태그한다. 
관심있는 어휘는 서술어(형용사, 동사형태 모두 포함), 감정 정도를 나타내는 부사(매우, 약간 조금 등등), 간접적으로 감정을 나타내는 명사 어휘, 그리고 주어와 목적어들이다.

감정 유닛들은 1차적으로 어휘 품사에 따라 태그를 한다. 이때는 명사에 관심을 두지않고, 주어와 목적어를 나타내는 조사 이/가/을/를 과 형용사와 부사에 태그를 한다.

2차적으로 명사 덩어리를 합친다. 주/목적어의 경우, 긴 주어는 앞의 EmoUnit에 우결합으로, '이/가' 로 부터 시작해서 앞에 있는 NN와 NNG 태그로 된 EmoUnit을 합친다.
합친 후 재료는 ArrayList의 remove 메소드를 사용해서 지운다. 합쳐진 후 ObjectWord또는 SubjectWord로 태그한다. 주어후보 목록을 만들 필요는 없다. 
(만약 주어가 없는 경우는 이 단계에서는 생략한다.)


3차적으로 부사-형용사 인접 수식관계를 파악한다. 이때는 아이템간의 위치를 전환한다.
아주 인접한 경우는 수식관계를 만들고 생략한다. (Enhancer/Reducer를 설정)
하지만 떨어져있는 경우에는 부사를 인접 수식관계가 만들어지지 않은 형용사의 인덱스 앞에 삽입한다. 후에 수식관계를 태그한다. 만약 dangling adjective가 없다면 생략한다.

4차적으로 형용사(이미 부사와의 위치는 정렬되었다고 생각하고)-명사의 수식 관계를 파악한다.
형용사의 경우는 명사 앞 형용사와 서술어 앞의 형용사로 나뉠 수 있다. 뒤에오는 문자의 품사 태그에 따라 결정한다. 여기에서 DescNext*와 DescSubject를 가를 수 있다.

5차적으로는 생략된 주어를 설정한다. 결합 관계를 파악하고도 주어 후보가 없다면 '나'를 넣는다.
(시험이 망해서 슬프다. =>
형용사와 주어만 보자면 시험-슬프다가 되지만, 뒤의 서술어와 '결합'을 고려하면 시험이 망하다로 이미 결합되었으므로, 슬프다는 Dangling adjective이다. 문장 어두에 '나'를 SubjectWord로 마크해서 넣어준다.)  



a)먼저 문장을 띄어쓰기 기준으로 자른다. 잘린 조각을 단어라고 부른다.
b)-이/-가/-은/-는 으로 끝나는 단어는 주어 추정 목록에 넣는다
c)-을/-를 로 끝나는 단어는 목적어 추정 목록에 넣는다.

2차 분석에서는 EmoUnit 간의 관계만으로 파악한다. 만약 '좀, 약간'이라는 표현이 서술어와 멀어져있는 경우는, 뒤에서부터 가장 가까운 서술어 앞으로 옮겨준다.
예) 정말 세상에서 사라졌으면 좋겠다. => 세상에서 사라졌으면 정말-좋겠다.

-'면'은 전제조건을 담을 수 있을 것이다. 2차 분석에서는 약간의 인과 관계(좋겠다-현재는 그렇지 않음)등을 파악하여 현재 감정값을 역으로 추출해낼 수 있다. (아직 멀었음)

만약 현재 단계까지 주어추정목록이 비어있다면, "나"를 추가한다. 만약 2차 분석 이후로도 Dangling(달랑거리는) 형용사가 있다면 '나'라는 주어를 추가하여 연결한다.

3차 분석에서는 형태소 분석된, 감정이 설정되지않은 명사들에 대해서 탐색을 진행한다. 값이 있다면 설정한다.

$$반환값$$ 아직 루프임

5단계: 태그된 어휘들에 대한 연산

4차 분석에서는 각 태그들의 연산을 사용해 감정값을 조절한다. 예를 들어 Enhancer가 발견된 다음에 나오는 서술어는 강화된다.

$$반환값$$ 아직 루프임

6단계: 감정값 정규화
감정 수치는 로그를 사용해서 정규화한다. 최소값은 0 (10base log, 1), 최대값은 2(10base log, 100)이다. 모든 감정 벡터는 양의 정수이다.
로그를 사용하는 이유는 감정값 상승에 대한 폭이, 선형함수에 비해서 적기 때문이다.

이 단계에서, 정수로 표현된 감정값들을 모두 상용로그로 정규화한다.

전체적으로 감정 벡터를 확인한 후, 네거티브가 강세인지 포지티브가 강세인지 파악한 다음에 강세인 쪽으로 보정을 한다. 보정 작업은 (강세 감정의 로그값 평균)*(0.1)^x 을 더해 전체적으로 조율한다.
이때 x는 문단 전체의 단어 수를 자연로그값을 정수로 내림한 값이다. 따라서 문단이 길어질 수록 보정에 추가된 값은 감소한다. 

이는 문단이 길어질수록 실질적으로 감정 정보를 거의 기억하지 못한다는 부분에 착안한다. (관련 논문 파악) 사람이 인지 가능한 정보에는 한계가 존재한다.

$$반환값$$ 감정값이 모두 설정되고, 값이 정규화된 EParagraph 객체

7단계: 분석 결과를 결과 객체로 변환
ResultProcessor 생성자에 EParagraph 객체를 넘겨주면서 JSON데이터를 생성한다.

8단계: 피드백, 모르는 어휘 새로 학습
이 작업에서 피드백된 결과는 다시 데이터베이스로 되돌려보낸다.
매번 학습된 결과는 데이터베이스내에 각 단어별로 각 행을 새롭게 구성하며, 감정값을 가져올 때는 이 값들을 모두 테이블에서 가져와 종합적인 값을 제공한다.
각 값은 시간에 비례하여 감소되는, 역비례 관계를 사용해 변환하여 사용한다.

시간이 흐를 수록 사람의 기억은 사라지며, 감정에 대한 정도도 감소하기 때문이다. 이렇게 하여, 과거에 대한 기억을 조금씩 잊고 새로운 형태를 반영할 수 있는 지능을 갖게 된다.

$$반환값$$ 없음
 


